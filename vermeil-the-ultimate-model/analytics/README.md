# Analytics

This directory contains all scripts and notebooks related to analyzing the performance of the Vermeil model. The goal is to provide a comprehensive suite of tools for understanding the model's behavior, identifying its strengths and weaknesses, and visualizing its performance.

## Directory Structure

-   **`dashboards/`**: This directory is intended to hold links to external monitoring dashboards, such as Weights & Biases or TensorBoard. This allows for easy access to real-time training metrics and visualizations.

-   **`reports/`**: This directory contains scripts that generate comprehensive performance reports. These reports may include metrics like FID scores, perplexity, and accuracy, as well as visualizations like performance plots and confusion matrices.

-   **`visualizations/`**: This directory is a storage space for any visualizations generated by the analytics scripts. This can include attention maps, confusion matrices, t-SNE plots of embeddings, and more.

## Usage

To use the scripts in this directory, you will typically need a trained model checkpoint and a dataset to evaluate it on. The scripts are designed to be run from the root of the `vermeil-the-ultimate-model` directory.

**Example:**

```bash
# Generate a performance report for a trained text model
python analytics/reports/performance_plotter.py --checkpoint_path models/checkpoints/text/best_model.pt --dataset_path data/processed/text/test.txt

# Generate an attention map visualization
python analytics/visualizations/attention_map.py --checkpoint_path models/checkpoints/multimodal/best_model.pt --image_path data/raw/images/sample.jpg --text "A sample caption"
```

## Contribution

We encourage contributions to this directory. If you have a new idea for a visualization, a new metric to track, or a better way to report performance, please feel free to open a pull request. When adding a new script, please make sure to include a clear description of what it does and how to use it.
